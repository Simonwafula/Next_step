# Audit Report (2026-02-06)

This document captures a repo-wide audit pass focused on correctness, security, performance, and maintainability.

## Scope
- Backend: FastAPI + SQLAlchemy + ingestion/processing + async tasks
- Repo hygiene: version control safety and reproducible dev setup
- Frontend: index/admin/dashboard rendering hardening (XSS mitigation)

## Key Findings (Before Fixes)

### Critical
- A local virtualenv (`backend/venv3.11/`) and a transient SQLite journal were tracked in git.
  - Impact: repo bloat, accidental dependency pinning, higher supply-chain risk, non-reproducible builds.

### High
- Unsafe embedding parsing via `eval()` in the search path.
  - Impact: if DB contents are compromised or attacker-controlled, this is a potential code execution vector.
- `DataProcessingService` referenced stale/non-existent `JobPost` fields (`created_at`, `description`, `is_active`, `role_family`).
  - Impact: background processing/statistics would crash at runtime.
- Multiple frontend pages rendered API-provided fields via `innerHTML` without escaping.
  - Impact: a malicious job title/org/location/url could inject script into the browser (XSS).

### Medium
- SQLAlchemy boolean comparisons used `== True/False` in query expressions across multiple modules.
  - Impact: inconsistent style; also easy to accidentally introduce Python-side truthiness mistakes.
- Formatting/lint drift: unused imports/variables, E402 in scripts/tests that intentionally mutate `sys.path`/env pre-import.
  - Impact: higher maintenance cost and lower signal-to-noise in review.

### Low
- Some tests behave like scripts (return values from tests, async tests skipped without markers).
  - Impact: weaker confidence; future pytest versions may hard-error on returned values.

## Changes Implemented
- Repo hygiene:
  - Stopped tracking `backend/venv3.11/` and removed a tracked SQLite journal.
  - Expanded `.gitignore` to cover virtualenv variants, SQLite `-journal`/WAL/SHM, and `artifacts/`.
- Embeddings:
  - Added `parse_embedding()` (`backend/app/ml/embeddings.py`) to safely parse embeddings from JSON strings or legacy Python-literal strings.
  - Removed `eval()` usage in search (`backend/app/services/search.py`).
  - Persist embeddings as JSON strings in the embeddings update task (`backend/app/tasks/processing_tasks.py`).
  - Updated embedding consumers (e.g., dedupe service) to parse embeddings instead of assuming in-memory lists.
- Backend correctness:
  - Updated `DataProcessingService` to align with current `JobPost` schema (`first_seen/last_seen/description_raw`), and derived “active” from recency rather than a missing `is_active` column.
- Consistency/quality:
  - Replaced SQLAlchemy `== True/False` comparisons with `.is_(True/False)`.
  - Added narrow `# ruff: noqa: E402` where scripts/tests intentionally set env or `sys.path` before imports.
  - Made backend `ruff` clean and applied `ruff format`.
- Frontend security:
  - Added `frontend/js/sanitize.js` and updated index/admin/dashboard scripts to escape strings and restrict links to `http(s)` (mitigates XSS when using template strings/`innerHTML`).

## Tests Run
- `backend/venv3.11/bin/ruff check backend`
- `backend/venv3.11/bin/ruff format backend --check`
- `backend/venv3.11/bin/pytest`

## Improvement Proposals

### 1) Make Dev Tooling Reproducible
- Problem: `ruff` is required for quality gates but not captured in a dev dependency file.
- Proposed solution: add `backend/requirements-dev.txt` (or migrate to `pyproject.toml`) and optionally add `pre-commit` hooks.
- Trade-offs/risks: minor onboarding work; keeps CI/dev environments consistent.
- Complexity: low
 - Implemented:
   - Added `backend/requirements-dev.txt` (pins `ruff`) and updated `AGENTS.md` dev bootstrap instructions.

### 2) Harden WhatsApp Webhook (Twilio)
- Problem: `/whatsapp/webhook` currently trusts inbound form posts.
- Proposed solution: validate Twilio signatures, enforce strict request size limits, and rate limit the endpoint.
- Trade-offs/risks: misconfig can reject valid requests; needs careful rollout and testing with Twilio.
- Complexity: medium
 - Implemented:
   - Added signature validation via `X-Twilio-Signature` using Twilio `RequestValidator` (controlled by `TWILIO_VALIDATE_WEBHOOK_SIGNATURE`, default true).
   - Added request size guard (rejects `Content-Length` > 100,000) and endpoint-specific rate limiting.
   - Added `TWILIO_WEBHOOK_URL` override and `x-forwarded-*` handling to validate behind proxies.

### 3) Normalize “Script Tests”
- Problem: several `backend/test_*.py` files act like runnable scripts; some async tests are skipped and some tests return values.
- Proposed solution: convert to proper tests with assertions + `pytest.mark.asyncio` where needed; move runnable scripts out of pytest collection.
- Trade-offs/risks: can expose real failures that were previously hidden; may need fixtures for DB state.
- Complexity: medium
 - Implemented:
   - Moved the runnable smoke scripts into `backend/scripts/` so pytest no longer collects them.
   - Added real, deterministic tests under `backend/tests/` with assertions (auth cookie flow, Twilio webhook validation, and vector type behavior).

### 4) Embeddings Storage Migration
- Problem: embeddings are stored in a text column; parsing is required at read-time; Postgres deployments will want `pgvector`.
- Proposed solution: migrate to `vector` (pgvector) in Postgres with an Alembic migration and dual-read during transition.
- Trade-offs/risks: migration complexity; must handle backfill and mixed states.
- Complexity: high (do not implement without explicit instruction)
 - Implemented:
   - Added `job_post.embedding_vector` (dual storage alongside the legacy `embedding` TEXT column).
   - Added a portable SQLAlchemy type (`VectorString`) that compiles to `vector(dim)` on Postgres and `TEXT` elsewhere.
   - Updated consumers to prefer `embedding_vector` when present (dual-read) and embedding update tasks to backfill `embedding_vector` cheaply from existing JSON before recomputing.
   - `init_db()` now ensures the `vector` extension (when possible), adds the column for existing deployments, and can optionally build an ANN index when `PGVECTOR_CREATE_INDEX=true`.
